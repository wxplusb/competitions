## Data Fusion 2023 - Adversarial ML между командами атакующих и защищающих ML модели на транзакционных данных.

### Задача «Защита»:
[ODS](https://ods.ai/tracks/data-fusion-2023-competitions/competitions/data-fusion2023-defence)

Имеется банковская RNN модель бинарной классификации (код и чекпойнт), предсказывающая не возврат кредита клиентом. Доступа к полному объему данных, на которых модель была обучена,  у вас нет. Однако, есть небольшая размеченная выборка клиентов. 
Известно, что на вход вашему разработанному решению будет подаваться два файла:
1) оригинальный файл с транзакциями клиентов банка. Для каждого клиента дано 300 транзакций, каждая транзакция представляет собой сумму, код, время транзакции.
2) атакованный файл - тот же оригинальный файл, но в нем по каждому клиенту могут быть изменены транзакции, атакующим разрешается изменить сумму и код не более чем у 10 транзакций по каждому клиенту. 

- Цель атакующих - как можно сильнее увеличить разницу между предсказаниями вашей модели на оригинальном и атакованном файле.
- Необходимо разработать модель бинарной классификации, которая будет устойчива к таким атакам (используя имеющуюся банковскую модель (которая не защищена от атак) и предоставленные вам размеченные данные транзакций).
- Метрика соревнования — Mean Harm ROC-AUC. Это среднее гармоническое ROC-AUC на исходных данных и на атакованных. Метрика сочетает в себе компромисс между повышением защищенности модели и потенциальным снижением ее качества. 

### Решение
Моя модель представляла блендинг двух моделей:
1) Бустинг catboost, обученный на фичах, извлеченных из размеченных данных, восновном таких, которые при атаке не сильно изменяются или вообще не изменяются - сглаженные счетчики кодов и временные фичи. Также в качестве фичи - использовалось предсказание имеющейся банковской модели. [1_train_cb.ipynb](1_train_cb.ipynb)
2) Банковская RNN модель, которая была дообучена с помощью adversarial training. Сначала для каждого клиента из тренировочных данных были сгенерированы атакованные транзакции. Дальше оригинальные и атакованные данные были объединены в один датасет, то есть в датасете присутствовал каждый клиент 2 раза с одинаковым таргетом, но с разными транзакциями. Модель, натренированная на таких данных, была более устойчива к атакам. [2_adversarial_train.ipynb](2_adversarial_train.ipynb)
- Перед блендингом прежде чем подать на вход двум моделям транзакции по каждому клиенту перемешивались и часть транзакций отбрасывалась ([Sample Shielding](https://arxiv.org/pdf/2205.01714.pdf)), что еще сильнее уменьшало эффект атаки и почти не влияло на предсказание на оригинальных транзакциях. [final_submit/run.py](final_submit/run.py)


### Место:

>>> | Турнир 1 | Турнир 2
-------- | -----| --------
Место  | 5 | 7


