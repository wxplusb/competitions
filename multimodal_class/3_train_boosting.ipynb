{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport src.my, src.net, src.data, src.models, src.text_utils\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import pickle\n",
    "\n",
    "# import net, data\n",
    "import src.text_utils as tu\n",
    "import src.my as my\n",
    "from src.my import p\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option(\"max_colwidth\", 45)\n",
    "pd.set_option(\"display.precision\", 1)\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "# pd.set_option(\"display.max_rows\", 5)\n",
    "# pd.reset_option(\"display.max_rows\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "dir_data = 'data/'\n",
    "dir_out = 'out/'\n",
    "os.makedirs(dir_out, exist_ok=True)\n",
    "\n",
    "SEED = 34\n",
    "N_CPU = os.cpu_count()\n",
    "\n",
    "np.random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>fold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>325286</td>\n",
       "      <td>251</td>\n",
       "      <td>493</td>\n",
       "      <td>электроника смартфоны телефоны аксессуары...</td>\n",
       "      <td>4</td>\n",
       "      <td>зарядный кабель borofone bx1 lightning ай...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>888134</td>\n",
       "      <td>748</td>\n",
       "      <td>6081</td>\n",
       "      <td>одежда женская одежда белье купальники трусы</td>\n",
       "      <td>3</td>\n",
       "      <td>трусы sela трусы слипы эластичного бесшов...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  category_id  shop_id  \\\n",
       "0      325286          251      493   \n",
       "1      888134          748     6081   \n",
       "\n",
       "                                  category_name  fold  \\\n",
       "0  электроника смартфоны телефоны аксессуары...     4   \n",
       "1  одежда женская одежда белье купальники трусы     3   \n",
       "\n",
       "                                           text  \n",
       "0  зарядный кабель borofone bx1 lightning ай...  \n",
       "1  трусы sela трусы слипы эластичного бесшов...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy = pd.read_parquet(dir_out+'prepared_df.pq')\n",
    "X_test = pd.read_parquet(dir_out+'prepared_test.pq')\n",
    "Xy[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = Xy['category_id'].unique().tolist()\n",
    "len(class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так выглядят эмбеддинги от берта и EfficientNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>bert0</th>\n",
       "      <th>bert1</th>\n",
       "      <th>bert2</th>\n",
       "      <th>bert3</th>\n",
       "      <th>bert4</th>\n",
       "      <th>bert5</th>\n",
       "      <th>bert6</th>\n",
       "      <th>bert7</th>\n",
       "      <th>bert8</th>\n",
       "      <th>...</th>\n",
       "      <th>bert758</th>\n",
       "      <th>bert759</th>\n",
       "      <th>bert760</th>\n",
       "      <th>bert761</th>\n",
       "      <th>bert762</th>\n",
       "      <th>bert763</th>\n",
       "      <th>bert764</th>\n",
       "      <th>bert765</th>\n",
       "      <th>bert766</th>\n",
       "      <th>bert767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>325286</td>\n",
       "      <td>1.629</td>\n",
       "      <td>0.429</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.358</td>\n",
       "      <td>1.440</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-1.702</td>\n",
       "      <td>1.529</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>...</td>\n",
       "      <td>1.979</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>0.370</td>\n",
       "      <td>1.162</td>\n",
       "      <td>2.046</td>\n",
       "      <td>0.587</td>\n",
       "      <td>1.470</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.530</td>\n",
       "      <td>2.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>888134</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-1.058</td>\n",
       "      <td>1.961</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.743</td>\n",
       "      <td>-1.077</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>1.623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460</td>\n",
       "      <td>1.588</td>\n",
       "      <td>1.023</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1.068</td>\n",
       "      <td>-1.224</td>\n",
       "      <td>0.858</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-0.238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  bert0  bert1  bert2  bert3  bert4  bert5  bert6  bert7  bert8  \\\n",
       "0      325286  1.629  0.429  1.014  1.358  1.440  0.094 -1.702  1.529 -0.759   \n",
       "1      888134  0.656  0.249 -1.058  1.961 -0.309  0.743 -1.077 -0.318  1.623   \n",
       "\n",
       "   ...  bert758  bert759  bert760  bert761  bert762  bert763  bert764  \\\n",
       "0  ...    1.979   -0.461    0.370    1.162    2.046    0.587    1.470   \n",
       "1  ...    0.460    1.588    1.023    0.386    0.743    1.068   -1.224   \n",
       "\n",
       "   bert765  bert766  bert767  \n",
       "0    0.950    0.530    2.958  \n",
       "1    0.858   -0.987   -0.238  \n",
       "\n",
       "[2 rows x 769 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = 0\n",
    "bert_train_val_embs = pd.read_parquet(f'out/bert_model/bert_train_val_embs_f{fold}.pq')\n",
    "bert_test_embs = pd.read_parquet(f'out/bert_model/bert_test_embs_f{fold}.pq')\n",
    "\n",
    "img_train_val_embs = pd.read_parquet(f'out/image_model/img_train_val_embs_f{fold}.pq')\n",
    "img_test_embs = pd.read_parquet(f'out/image_model/img_test_embs_f{fold}.pq')\n",
    "\n",
    "bert_train_val_embs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((91120, 769), (91120, 1281))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_train_val_embs.shape, img_train_val_embs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyboost c PCA и Umap: f1 ~ 0.875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_boost import GradientBoosting, TLPredictor, TLCompiledPredictor\n",
    "from py_boost.cv import CrossValidation\n",
    "\n",
    "from py_boost.gpu.losses.multiclass_metrics import MultiF1Score\n",
    "import cupy as cp\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class F1Weighted(MultiF1Score):\n",
    "    \"\"\"CrossEntropy Metric for the multiclassification task\"\"\"\n",
    "    alias = 'F1Weighted'\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        return f1_score(y_true.get(), y_pred.argmax(axis=1).get(), average='weighted')\n",
    "\n",
    "fold = 0\n",
    "\n",
    "both_reducer = PCA(n_components=50)\n",
    "\n",
    "Xy_train = Xy.loc[Xy.fold!=fold,['product_id','shop_title', 'category_id']]\n",
    "\n",
    "cats_in_train = Xy_train['category_id'].unique()\n",
    "\n",
    "Xy_val = Xy.loc[(Xy.fold==fold) & (Xy.category_id.isin(cats_in_train)),['product_id','shop_title', 'category_id']]\n",
    "\n",
    "bert_train_val_embs = pd.read_parquet(f'out/bert_model/bert_train_val_embs_f{fold}.pq')\n",
    "img_train_val_embs = pd.read_parquet(f'out/image_model/img_train_val_embs_f{fold}.pq')\n",
    "\n",
    "Xy_train = Xy_train.merge(bert_train_val_embs,on='product_id').merge(img_train_val_embs,on='product_id')\n",
    "\n",
    "Xy_val = Xy_val.merge(bert_train_val_embs,on='product_id').merge(img_train_val_embs,on='product_id')\n",
    "\n",
    "train_umap = both_reducer.fit_transform(Xy_train.iloc[:,3:])\n",
    "train_umap = pd.DataFrame(train_umap)\n",
    "my.flat_cols(train_umap,'both')\n",
    "\n",
    "val_umap = both_reducer.transform(Xy_val.iloc[:,3:])\n",
    "val_umap = pd.DataFrame(val_umap)\n",
    "my.flat_cols(val_umap,'both')    \n",
    "\n",
    "Xy_train = pd.concat([Xy_train.iloc[:,:3],train_umap],axis=1)\n",
    "Xy_val = pd.concat([Xy_val.iloc[:,:3],val_umap],axis=1)\n",
    "\n",
    "cols = ['product_id','category_id','shop_title']\n",
    "\n",
    "X_train=Xy_train.drop(columns=cols).to_numpy()\n",
    "y_train=Xy_train['category_id'].to_numpy()\n",
    "\n",
    "X_val=Xy_val.drop(columns=cols).to_numpy()\n",
    "y_val=Xy_val['category_id'].to_numpy()\n",
    "\n",
    "model = GradientBoosting('crossentropy',metric=F1Weighted(),ntrees=10000, lr=0.03, verbose=5, es=50, lambda_l2=1, gd_steps=1,\n",
    "                         subsample=1, colsample=1, min_data_in_leaf=10, use_hess=True,\n",
    "                         max_bin=256, max_depth=6)\n",
    "\n",
    "model.fit(X_train, y_train, eval_sets = [{'X': X_val, 'y': y_val}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U cupy-cuda11x py-boost\n",
    "import umap\n",
    "from py_boost import GradientBoosting, TLPredictor, TLCompiledPredictor\n",
    "from py_boost.cv import CrossValidation\n",
    "\n",
    "fold = 0\n",
    "\n",
    "both_reducer = umap.UMAP(n_components=50,\n",
    "                  random_state=SEED,\n",
    "                  low_memory=False,\n",
    "                  metric='cosine',\n",
    "                  verbose=True)\n",
    "\n",
    "Xy_train = Xy.loc[Xy.fold!=fold,['product_id','shop_title', 'category_id']]\n",
    "\n",
    "cats_in_train = Xy_train['category_id'].unique()\n",
    "\n",
    "Xy_val = Xy.loc[(Xy.fold==fold) & (Xy.category_id.isin(cats_in_train)),['product_id','shop_title', 'category_id']]\n",
    "\n",
    "bert_train_val_embs = pd.read_parquet(f'out/bert_model/bert_train_val_embs_f{fold}.pq')\n",
    "img_train_val_embs = pd.read_parquet(f'out/image_model/img_train_val_embs_f{fold}.pq')\n",
    "\n",
    "Xy_train = Xy_train.merge(bert_train_val_embs,on='product_id').merge(img_train_val_embs,on='product_id')\n",
    "\n",
    "Xy_val = Xy_val.merge(bert_train_val_embs,on='product_id').merge(img_train_val_embs,on='product_id')\n",
    "\n",
    "train_umap = both_reducer.fit_transform(Xy_train.iloc[:,3:])\n",
    "train_umap = pd.DataFrame(train_umap)\n",
    "my.flat_cols(train_umap,'both')\n",
    "\n",
    "val_umap = both_reducer.transform(Xy_val.iloc[:,3:])\n",
    "val_umap = pd.DataFrame(val_umap)\n",
    "my.flat_cols(val_umap,'both')    \n",
    "\n",
    "Xy_train = pd.concat([Xy_train.iloc[:,:3],train_umap],axis=1)\n",
    "Xy_val = pd.concat([Xy_val.iloc[:,:3],val_umap],axis=1)\n",
    "\n",
    "cols = ['product_id','category_id','shop_title']\n",
    "\n",
    "X_train=Xy_train.drop(columns=cols).to_numpy()\n",
    "y_train=Xy_train['category_id'].to_numpy()\n",
    "\n",
    "X_val=Xy_val.drop(columns=cols).to_numpy()\n",
    "y_val=Xy_val['category_id'].to_numpy()\n",
    "\n",
    "model = GradientBoosting('crossentropy',metric='f1',\n",
    "                         ntrees=10000, lr=0.03, verbose=5, es=50, lambda_l2=1, gd_steps=1,\n",
    "                         subsample=1, colsample=1, min_data_in_leaf=10, use_hess=True,\n",
    "                         max_bin=256, max_depth=6)\n",
    "\n",
    "model.fit(X_train, y_train, eval_sets = [{'X': X_val, 'y': y_val}])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catboost f1 ~ 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fold = 0\n",
    "\n",
    "both_reducer = PCA(n_components=100)\n",
    "\n",
    "class_names = Xy['category_id'].unique().tolist()\n",
    "\n",
    "Xy_train = Xy.loc[Xy.fold!=fold,['product_id','shop_title', 'category_id']]\n",
    "\n",
    "cats_in_train = Xy_train['category_id'].unique()\n",
    "\n",
    "Xy_val = Xy.loc[(Xy.fold==fold) & (Xy.category_id.isin(cats_in_train)),['product_id','shop_title', 'category_id']]\n",
    "\n",
    "bert_train_val_embs = pd.read_parquet(f'out/bert_model/bert_train_val_embs_f{fold}.pq')\n",
    "img_train_val_embs = pd.read_parquet(f'out/image_model/img_train_val_embs_f{fold}.pq')\n",
    "\n",
    "Xy_train = Xy_train.merge(bert_train_val_embs,on='product_id').merge(img_train_val_embs,on='product_id')\n",
    "\n",
    "Xy_val = Xy_val.merge(bert_train_val_embs,on='product_id').merge(img_train_val_embs,on='product_id')\n",
    "\n",
    "train_umap = both_reducer.fit_transform(Xy_train.iloc[:,3:])\n",
    "train_umap = pd.DataFrame(train_umap)\n",
    "my.flat_cols(train_umap,'both')\n",
    "\n",
    "val_umap = both_reducer.transform(Xy_val.iloc[:,3:])\n",
    "val_umap = pd.DataFrame(val_umap)\n",
    "my.flat_cols(val_umap,'both')    \n",
    "\n",
    "Xy_train = pd.concat([Xy_train.iloc[:,:3],train_umap],axis=1)\n",
    "Xy_val = pd.concat([Xy_val.iloc[:,:3],val_umap],axis=1)\n",
    "\n",
    "cols = ['product_id','category_id']\n",
    "\n",
    "X_train=Xy_train.drop(columns=cols)\n",
    "y_train=Xy_train['category_id']\n",
    "\n",
    "X_val=Xy_val.drop(columns=cols)\n",
    "y_val=Xy_val['category_id']\n",
    "\n",
    "cols = ['product_id','category_id']\n",
    "\n",
    "train_pool = Pool(data=Xy_train.drop(columns=cols), label=Xy_train['category_id'],cat_features=['shop_title'])\n",
    "\n",
    "val_pool = Pool(data=Xy_val.drop(columns=cols), label=Xy_val['category_id'],cat_features=['shop_title'])\n",
    "\n",
    "del Xy_train,Xy_val, bert_train_val_embs, img_train_val_embs\n",
    "gc.collect()\n",
    "\n",
    "LR = None\n",
    "\n",
    "cb = CatBoostClassifier(iterations=10000,learning_rate=LR,random_seed=SEED, early_stopping_rounds=50,eval_metric='TotalF1:average=Weighted', class_names=class_names)\n",
    "\n",
    "cb.fit(train_pool, eval_set=val_pool, verbose=1)\n",
    "    # cb_models.append(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "import umap\n",
    "\n",
    "both_reducer = umap.UMAP(n_components=50,\n",
    "                  random_state=SEED,\n",
    "                  low_memory=False,\n",
    "                  metric='cosine',\n",
    "                  verbose=True)\n",
    "\n",
    "LR = None\n",
    "\n",
    "for fold in sorted(Xy['fold'].unique()):\n",
    "    Xy_train = Xy.loc[Xy.fold!=fold,['product_id','shop_title', 'category_id']]\n",
    "\n",
    "    cats_in_train = Xy_train['category_id'].unique()\n",
    "\n",
    "    Xy_val = Xy.loc[(Xy.fold==fold) & (Xy.category_id.isin(cats_in_train)),['product_id','shop_title', 'category_id']]\n",
    "\n",
    "    bert_train_val_embs = pd.read_parquet(f'out/bert_model/bert_train_val_embs_f{fold}.pq')\n",
    "    img_train_val_embs = pd.read_parquet(f'out/image_model/img_train_val_embs_f{fold}.pq')\n",
    "\n",
    "    Xy_train = Xy_train.merge(bert_train_val_embs,on='product_id').merge(img_train_val_embs,on='product_id')\n",
    "\n",
    "    Xy_val = Xy_val.merge(bert_train_val_embs,on='product_id').merge(img_train_val_embs,on='product_id')\n",
    "\n",
    "    train_umap = both_reducer.fit_transform(Xy_train.iloc[:,3:])\n",
    "    train_umap = pd.DataFrame(train_umap)\n",
    "    my.flat_cols(train_umap,'both')\n",
    "\n",
    "    val_umap = both_reducer.transform(Xy_val.iloc[:,3:])\n",
    "    val_umap = pd.DataFrame(val_umap)\n",
    "    my.flat_cols(val_umap,'both')    \n",
    "\n",
    "    Xy_train = pd.concat([Xy_train.iloc[:,:3],train_umap],axis=1)\n",
    "    Xy_val = pd.concat([Xy_val.iloc[:,:3],val_umap],axis=1)\n",
    "\n",
    "    cols = ['product_id','category_id']\n",
    "\n",
    "    train_pool = Pool(data=Xy_train.drop(columns=cols), label=Xy_train['category_id'],cat_features=['shop_title'])\n",
    "\n",
    "    val_pool = Pool(data=Xy_val.drop(columns=cols), label=Xy_val['category_id'],cat_features=['shop_title'])\n",
    "\n",
    "    del Xy_train,Xy_val, bert_train_val_embs, img_train_val_embs\n",
    "    gc.collect()\n",
    "\n",
    "    cb = CatBoostClassifier(iterations=10000,learning_rate=LR,random_seed=SEED, task_type=\"GPU\",early_stopping_rounds=50,eval_metric='TotalF1:average=Weighted', gpu_ram_part=0.8, class_names=class_names, boosting_type='Plain')\n",
    "\n",
    "    cb.fit(train_pool, eval_set=val_pool, verbose=1)\n",
    "    # cb_models.append(cb)\n",
    "  \n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "отдельный reducer для каждого эмбеддинга не улучшал результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_reducer = umap.UMAP(n_components=10,\n",
    "                  random_state=SEED,\n",
    "                  low_memory=False,\n",
    "                  metric='cosine',\n",
    "                  verbose=True)\n",
    "\n",
    "img_reducer = umap.UMAP(n_components=10,\n",
    "                  random_state=SEED,\n",
    "                  low_memory=False,\n",
    "                  metric='cosine',\n",
    "                  verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('dl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddfe809504c16a523e85c666b643af5a9bc21e5032e1e75cef76cdb438e9b837"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
